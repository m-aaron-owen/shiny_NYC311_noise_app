shiny::runApp()
shiny::runApp()
shiny::runApp()
runApp()
shiny::runApp()
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
install.packages("ISLR")
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
head(Hitters)
library(glmnet)
install.packages("glmnet")
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
grid = 10^seq(5, -2, length = 100)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
coef(ridge.models) #Inspecting the various coefficient estimates.
ridge.models$lambda[80] #Lambda = 0.2595.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
ridge.models$lambda[80] #Lambda = 0.2595.
predict(ridge.models, s = 50, type = "coefficients")
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ]) # arbritrary lambda
mean((ridge.lambda5 - y.test)^2)
ridge.largelambda = predict(ridge.models.train, s = 1e10, newx = x[test, ])
mean((ridge.largelambda - y.test)^2)
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
ridge.bestlambdatrain = predict(ridge.models.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
library(caret)
set.seed(0)
train_control = trainControl(method = 'cv', number=10)
tune.grid = expand.grid(lambda = grid, alpha=c(0))
ridge.caret = train(x[train, ], y[train],
method = 'glmnet',
trControl = train_control, tuneGrid = tune.grid)
### Plot the tuning object:
plot(ridge.caret, xTrans=log)
plot(ridge.caret)#, xTrans=log)
plot(ridge.caret, xTrans=log)
pred = predict.train(ridge.caret, newdata = x[test,])
mean((pred - y[test])^2)
lasso.models = glmnet(x, y, alpha = 1, lambda = grid)
dim(coef(lasso.models)) #20 different coefficients, estimated 100 times --
coef(lasso.models) #Inspecting the various coefficient estimates.
lasso.models$lambda[80] #Lambda = 0.2595.
coef(lasso.models)[, 80] #Most estimates not close to 0.
sum(abs(coef(lasso.models)[-1, 80])) #L1 norm is 228.1008.
lasso.models$lambda[15] #Lambda = 10,235.31.
coef(lasso.models)[, 15] #Estimates all 0.
sum(abs(coef(lasso.models)[-1, 15])) #L1 norm is essentially 0.
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
predict(lasso.models, s = 50, type = "coefficients")
lasso.models.train = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
lasso.lambda5 = predict(lasso.models.train, s = 5, newx = x[test, ])
mean((lasso.lambda5 - y.test)^2)
set.seed(0)
cv.lasso.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 1, nfolds = 10)
plot(cv.lasso.out, main = "Lasso Regression\n")
bestlambda.lasso
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
log(bestlambda.lasso)
lasso.bestlambdatrain = predict(lasso.models.train, s = bestlambda.lasso, newx = x[test, ])
mean((lasso.bestlambdatrain - y.test)^2)
set.seed(0)
train_control = trainControl(method = 'cv', number=10)
tune.grid = expand.grid(lambda = grid, alpha=c(1))
ridge.caret = train(x[train, ], y[train],
method = 'glmnet',
trControl = train_control, tuneGrid = tune.grid)
### Plot the tuning object:
plot(ridge.caret, xTrans=log)
### We see caret::train returns a different result from the
### one by cv.glmnet. By comparing the ridge.caret$results
### and cv.ridge.out$cvm, it's most likely to be rounding and
### averaging.
### Predicting with the final model
pred = predict.train(ridge.caret, newdata = x[test,])
mean((pred - y[test])^2)
### Note: there is a "finalModel in ridge.caret. But unfortunately, using it
###       for predicting often results in error.There is some issue with the
###       compactibility of the "predict" function and the model from caret.train
predict(ridge.caret$finalModel, newdata = x[test,])
plot(ridge.caret, xTrans=log)
pred = predict.train(ridge.caret, newdata = x[test,])
mean((pred - y[test])^2)
predict(ridge.caret$finalModel, newdata = x[test,])
